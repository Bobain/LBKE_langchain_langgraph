{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UROQ8Cs_ridHRXzZp5iAtltWTZI0Q1GR",
      "authorship_tag": "ABX9TyOfeVxkzq0qgX0NKFRX1h21",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobain/LBKE_langchain_langgraph/blob/main/notebooks/Formation_LangChain_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ody4VWFZip48",
        "outputId": "aab0c50e-f8db-4ffd-925d-55529b9d1aff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Importing environement variables : will work only this way in colab (harcoded path inside my google drive)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/drive/MyDrive/env_var/.env\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[mistralai]\""
      ],
      "metadata": {
        "id": "AxHK1xzYrT6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "messages = [\n",
        "HumanMessage(\"Génère un court programme Python avec LangChain qui envoie un message à un LLM.\"),\n",
        "]\n",
        "res=model.invoke(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzXrHCQL7ugf",
        "outputId": "26d42445-15b5-4f01-fadf-a7ada517bc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voici un court programme Python utilisant LangChain pour envoyer un message à un LLM (comme OpenAI, Hugging Face, etc.) :\n",
            "\n",
            "```python\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.chains import LLMChain\n",
            "\n",
            "# Initialiser le modèle LLM (nécessite une clé API OpenAI)\n",
            "llm = OpenAI(temperature=0.7)  # temperature contrôle la créativité\n",
            "\n",
            "# Créer un template de prompt\n",
            "prompt_template = PromptTemplate(\n",
            "    input_variables=[\"question\"],\n",
            "    template=\"Réponds à la question suivante : {question}\"\n",
            ")\n",
            "\n",
            "# Créer une chaîne LLM\n",
            "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
            "\n",
            "# Envoyer une question et obtenir une réponse\n",
            "question = \"Qu'est-ce que LangChain ?\"\n",
            "response = chain.run(question)\n",
            "\n",
            "print(f\"Question : {question}\")\n",
            "print(f\"Réponse : {response}\")\n",
            "```\n",
            "\n",
            "### Prérequis :\n",
            "1. Installez LangChain et OpenAI :\n",
            "   ```bash\n",
            "   pip install langchain openai\n",
            "   ```\n",
            "2. Configurez votre clé API OpenAI :\n",
            "   ```python\n",
            "   import os\n",
            "   os.environ[\"OPENAI_API_KEY\"] = \"votre-clé-api-ici\"\n",
            "   ```\n",
            "\n",
            "### Version alternative avec Hugging Face :\n",
            "Si vous préférez utiliser Hugging Face, remplacez la ligne du LLM par :\n",
            "```python\n",
            "from langchain.llms import HuggingFaceHub\n",
            "llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\": 0.7})\n",
            "```\n",
            "\n",
            "N'oubliez pas de configurer votre clé API Hugging Face si nécessaire.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Initialiser le LLM (nécessite une clé API OpenAI)\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# Créer un template de prompt\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Réponds brièvement à la question suivante : {question}\"\n",
        ")\n",
        "\n",
        "# Créer une chaîne de traitement\n",
        "chain = template | llm\n",
        "\n",
        "# Poser une question et obtenir une réponse\n",
        "question = \"Quel est le capital de la France ?\"\n",
        "response = chain.invoke({\"question\": question})\n",
        "\n",
        "print(f\"Question : {question}\")\n",
        "print(f\"Réponse : {response}\")\n",
        "\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xxl\",\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "MWGRe63d79RP",
        "outputId": "a3466905-a64b-4bdd-91af-bee9050a8b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.llms'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1702012193.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialiser le LLM (nécessite une clé API OpenAI)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.llms'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TxQRysQHL1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown;\n",
        "Markdown(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "mSofAajf8NZo",
        "outputId": "c9682225-5e8a-48c1-ee75-a60d1c14c3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Voici un court programme Python utilisant LangChain pour envoyer un message à un LLM (comme OpenAI, Hugging Face, etc.) :\n\n```python\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# Initialiser le modèle LLM (nécessite une clé API OpenAI)\nllm = OpenAI(temperature=0.7)  # temperature contrôle la créativité\n\n# Créer un template de prompt\nprompt_template = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"Réponds à la question suivante : {question}\"\n)\n\n# Créer une chaîne LLM\nchain = LLMChain(llm=llm, prompt=prompt_template)\n\n# Envoyer une question et obtenir une réponse\nquestion = \"Qu'est-ce que LangChain ?\"\nresponse = chain.run(question)\n\nprint(f\"Question : {question}\")\nprint(f\"Réponse : {response}\")\n```\n\n### Prérequis :\n1. Installez LangChain et OpenAI :\n   ```bash\n   pip install langchain openai\n   ```\n2. Configurez votre clé API OpenAI :\n   ```python\n   import os\n   os.environ[\"OPENAI_API_KEY\"] = \"votre-clé-api-ici\"\n   ```\n\n### Version alternative avec Hugging Face :\nSi vous préférez utiliser Hugging Face, remplacez la ligne du LLM par :\n```python\nfrom langchain.llms import HuggingFaceHub\nllm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\": 0.7})\n```\n\nN'oubliez pas de configurer votre clé API Hugging Face si nécessaire."
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "messages = [\n",
        "HumanMessage(\"Generate a Python program with LangChain that send a message to an LLM.\"),\n",
        "]\n",
        "res=model.invoke(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMX9EXDiHOb1",
        "outputId": "09794171-effa-4c40-c4ef-cb0153b41485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Python Program with LangChain to Send a Message to an LLM\n",
            "\n",
            "Here's a complete Python program that uses LangChain to send a message to a language model (LLM). This example uses OpenAI's API, but you can easily adapt it for other LLMs supported by LangChain.\n",
            "\n",
            "```python\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.chains import LLMChain\n",
            "import os\n",
            "\n",
            "def main():\n",
            "    # Set your OpenAI API key (you can also set it as an environment variable)\n",
            "    # os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
            "\n",
            "    # Check if API key is set\n",
            "    if \"OPENAI_API_KEY\" not in os.environ:\n",
            "        print(\"Please set your OPENAI_API_KEY environment variable\")\n",
            "        return\n",
            "\n",
            "    # Initialize the OpenAI LLM\n",
            "    llm = OpenAI(\n",
            "        temperature=0.7,  # Controls randomness (0.0 to 1.0)\n",
            "        model_name=\"gpt-3.5-turbo\"  # You can change this to other models\n",
            "    )\n",
            "\n",
            "    # Create a prompt template\n",
            "    prompt_template = PromptTemplate(\n",
            "        input_variables=[\"topic\"],\n",
            "        template=\"Please provide a detailed explanation about {topic}.\"\n",
            "    )\n",
            "\n",
            "    # Create the LLM chain\n",
            "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
            "\n",
            "    # Get user input\n",
            "    topic = input(\"Enter a topic you want to learn about: \")\n",
            "\n",
            "    # Run the chain with the user's input\n",
            "    response = chain.run(topic)\n",
            "\n",
            "    # Print the response\n",
            "    print(\"\\nResponse from the LLM:\")\n",
            "    print(response)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "```\n",
            "\n",
            "## How to Use This Program:\n",
            "\n",
            "1. **Prerequisites**:\n",
            "   - Install required packages: `pip install langchain openai`\n",
            "   - Get an OpenAI API key from https://platform.openai.com/account/api-keys\n",
            "\n",
            "2. **Running the program**:\n",
            "   - Save the code to a file (e.g., `llm_chat.py`)\n",
            "   - Run it with: `python llm_chat.py`\n",
            "   - Enter a topic when prompted\n",
            "\n",
            "3. **Customization Options**:\n",
            "   - Change the `model_name` to use different OpenAI models\n",
            "   - Adjust the `temperature` parameter to control response randomness\n",
            "   - Modify the prompt template to change the question format\n",
            "\n",
            "## Alternative Version (Simpler)\n",
            "\n",
            "If you prefer a simpler version without LangChain's chaining:\n",
            "\n",
            "```python\n",
            "from langchain.llms import OpenAI\n",
            "import os\n",
            "\n",
            "def simple_llm_chat():\n",
            "    # Set your API key\n",
            "    if \"OPENAI_API_KEY\" not in os.environ:\n",
            "        print(\"Please set your OPENAI_API_KEY environment variable\")\n",
            "        return\n",
            "\n",
            "    llm = OpenAI(temperature=0.7)\n",
            "\n",
            "    while True:\n",
            "        user_input = input(\"\\nYou: (type 'quit' to exit) \")\n",
            "        if user_input.lower() == 'quit':\n",
            "            break\n",
            "\n",
            "        response = llm(user_input)\n",
            "        print(\"AI:\", response)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    simple_llm_chat()\n",
            "```\n",
            "\n",
            "This simpler version allows for a continuous chat session with the LLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "def main():\n",
        "    # Initialize the OpenAI LLM (you'll need to set your API key)\n",
        "    llm = OpenAI(temperature=0.7)\n",
        "\n",
        "    # Create a prompt template\n",
        "    template = \"\"\"Question: {question}\n",
        "\n",
        "    Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"question\"],\n",
        "        template=template,\n",
        "    )\n",
        "\n",
        "    # Create the LLM chain\n",
        "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    # Get user input\n",
        "    user_question = input(\"What would you like to ask the AI? \")\n",
        "\n",
        "    # Generate response\n",
        "    response = llm_chain.run(user_question)\n",
        "\n",
        "    # Print the response\n",
        "    print(\"\\nAI Response:\")\n",
        "    print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "LBWnITDnIPac",
        "outputId": "457abcbb-d3a8-4539-af53-f1d54b2e82df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.llms'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1186827369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.llms'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a seasonned python developer with high expertise on using LLMs\"\n",
        "prompt_message_sending = \"Generate a Python program with LangChain that send a message to an LLM.\""
      ],
      "metadata": {
        "id": "4xhLS9aVJhPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "human_message = HumanMessage(content=prompt_message_sending)\n",
        "system_message = SystemMessage(content=system_prompt)\n",
        "model.invoke([\n",
        "  system_message,\n",
        "  human_message,\n",
        "]).pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3OXDndvIWo5",
        "outputId": "e96eb3e7-bdb2-4b92-8fbc-a13e53e06c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Certainly! Below is a Python program that uses LangChain to send a message to a Language Model (LLM). This example assumes you have access to an LLM API, such as OpenAI's GPT-3.5 or GPT-4.\n",
            "\n",
            "First, ensure you have the necessary libraries installed. You can install them using pip:\n",
            "\n",
            "```bash\n",
            "pip install langchain openai\n",
            "```\n",
            "\n",
            "Here's the Python program:\n",
            "\n",
            "```python\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.chains import LLMChain\n",
            "\n",
            "# Initialize the OpenAI LLM\n",
            "llm = OpenAI(temperature=0.7)\n",
            "\n",
            "# Define a prompt template\n",
            "prompt_template = PromptTemplate(\n",
            "    input_variables=[\"message\"],\n",
            "    template=\"You are a helpful assistant. Respond to the following message: {message}\"\n",
            ")\n",
            "\n",
            "# Create an LLMChain\n",
            "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
            "\n",
            "# Define the message to send to the LLM\n",
            "message = \"Hello, how are you today?\"\n",
            "\n",
            "# Run the chain with the message\n",
            "response = chain.run(message)\n",
            "\n",
            "# Print the response\n",
            "print(\"Response from LLM:\", response)\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "\n",
            "1. **Import Libraries**: We import the necessary modules from LangChain and OpenAI.\n",
            "2. **Initialize the LLM**: We initialize the OpenAI LLM with a temperature setting. The temperature parameter controls the randomness of the output.\n",
            "3. **Define a Prompt Template**: We create a prompt template that includes a placeholder for the message we want to send to the LLM.\n",
            "4. **Create an LLMChain**: We create an LLMChain, which combines the LLM and the prompt template.\n",
            "5. **Define the Message**: We define the message we want to send to the LLM.\n",
            "6. **Run the Chain**: We run the LLMChain with the message and get the response from the LLM.\n",
            "7. **Print the Response**: Finally, we print the response from the LLM.\n",
            "\n",
            "### Note:\n",
            "- Make sure you have your OpenAI API key set up in your environment variables. You can set it using:\n",
            "  ```bash\n",
            "  export OPENAI_API_KEY='your-api-key'\n",
            "  ```\n",
            "- If you are using a different LLM provider, you can replace the `OpenAI` class with the appropriate class from LangChain (e.g., `HuggingFaceHub` for Hugging Face models).\n",
            "\n",
            "This program provides a basic structure for sending a message to an LLM using LangChain. You can extend it further by adding more complex prompts, chaining multiple LLMs, or integrating it with other components of LangChain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AI_MESSAGE = \"\"\"Sure, I'd be happy to help you create a Python program using LangChain to send a message to a Language Model (LLM). Here's a simple example:\n",
        "\n",
        "Please note that you'll need to have LangChain and the necessary LLM provider's package installed in your Python environment. For this example, I'll assume you're using OpenAI's GPT-3.5 model.\n",
        "\n",
        "First, install the required packages:\n",
        "\n",
        "```bash\n",
        "pip install langchain openai\n",
        "```\n",
        "\n",
        "Here's the Python program:\n",
        "\n",
        "```python\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"message\"],\n",
        "    template=\"You are a helpful assistant. Respond to the following message: {message}\"\n",
        ")\n",
        "\n",
        "# Create the LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Define the message you want to send\n",
        "message = \"Hello, how are you today?\"\n",
        "\n",
        "# Run the chain with the message\n",
        "response = chain.run(message)\n",
        "\n",
        "# Print the response\n",
        "print(response)\n",
        "```\n",
        "\n",
        "This program does the following:\n",
        "\n",
        "1. Imports the necessary classes from the LangChain library.\n",
        "2. Initializes the OpenAI LLM with a temperature of 0.9 (you can adjust this value to control the randomness of the output).\n",
        "3. Creates a prompt template that includes a placeholder for the message you want to send.\n",
        "4. Creates an LLMChain with the LLM and the prompt template.\n",
        "5. Defines the message you want to send to the LLM.\n",
        "6. Runs the chain with the message and stores the response.\n",
        "7. Prints the response.\n",
        "\n",
        "Before running this program, make sure you have set up your OpenAI API key. You can do this by creating a `.env` file in your project directory with the following content:\n",
        "\n",
        "```\n",
        "OPENAI_API_KEY=your_api_key_here\n",
        "```\n",
        "\n",
        "Replace `your_api_key_here` with your actual OpenAI API key. The LangChain library will automatically read this file and use the API key for authentication.\"\"\""
      ],
      "metadata": {
        "id": "J3BYtEg2JCyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "ai_message = AIMessage(AI_MESSAGE)"
      ],
      "metadata": {
        "id": "rC4z7qMLLY3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "model.invoke([\n",
        "  SystemMessage(content=system_prompt),\n",
        "  HumanMessage(content=prompt_message_sending),\n",
        "  ai_message,\n",
        "  HumanMessage(content=\"Please explain the code you generated\")\n",
        "]).pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOVH9P49LuEA",
        "outputId": "5fbe89a8-9648-4788-ce9a-6d2889c50cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Certainly! Let's break down the Python program step by step to understand how it works with LangChain to send a message to an LLM (Language Model).\n",
            "\n",
            "### Importing Required Classes\n",
            "\n",
            "```python\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.chains import LLMChain\n",
            "```\n",
            "\n",
            "- **`OpenAI`**: This class from the `langchain.llms` module is used to initialize and interact with OpenAI's language models.\n",
            "- **`PromptTemplate`**: This class from the `langchain.prompts` module is used to create templates for prompts. Prompts are the inputs given to the language model to generate a response.\n",
            "- **`LLMChain`**: This class from the `langchain.chains` module is used to create a chain that connects a language model with a prompt template. Chains in LangChain are sequences of operations that can be executed together.\n",
            "\n",
            "### Initializing the OpenAI LLM\n",
            "\n",
            "```python\n",
            "llm = OpenAI(temperature=0.9)\n",
            "```\n",
            "\n",
            "- **`OpenAI(temperature=0.9)`**: This initializes the OpenAI language model with a temperature of 0.9. The temperature parameter controls the randomness of the model's output. A higher temperature (closer to 1) makes the output more random, while a lower temperature (closer to 0) makes it more deterministic.\n",
            "\n",
            "### Creating a Prompt Template\n",
            "\n",
            "```python\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"message\"],\n",
            "    template=\"You are a helpful assistant. Respond to the following message: {message}\"\n",
            ")\n",
            "```\n",
            "\n",
            "- **`PromptTemplate`**: This creates a prompt template with a placeholder for the message.\n",
            "  - **`input_variables=[\"message\"]`**: This specifies that the prompt template has one input variable named \"message\".\n",
            "  - **`template=\"You are a helpful assistant. Respond to the following message: {message}\"`**: This is the actual template string. The `{message}` placeholder will be replaced with the actual message when the prompt is generated.\n",
            "\n",
            "### Creating the LLMChain\n",
            "\n",
            "```python\n",
            "chain = LLMChain(llm=llm, prompt=prompt)\n",
            "```\n",
            "\n",
            "- **`LLMChain(llm=llm, prompt=prompt)`**: This creates an LLMChain that connects the language model (`llm`) with the prompt template (`prompt`). The chain will use the prompt template to generate a prompt and then pass it to the language model to get a response.\n",
            "\n",
            "### Defining the Message\n",
            "\n",
            "```python\n",
            "message = \"Hello, how are you today?\"\n",
            "```\n",
            "\n",
            "- **`message = \"Hello, how are you today?\"`**: This defines the message you want to send to the language model.\n",
            "\n",
            "### Running the Chain with the Message\n",
            "\n",
            "```python\n",
            "response = chain.run(message)\n",
            "```\n",
            "\n",
            "- **`chain.run(message)`**: This runs the chain with the given message. The chain will:\n",
            "  1. Use the prompt template to generate a prompt by replacing the `{message}` placeholder with the actual message.\n",
            "  2. Pass the generated prompt to the language model to get a response.\n",
            "  3. Return the response from the language model.\n",
            "\n",
            "### Printing the Response\n",
            "\n",
            "```python\n",
            "print(response)\n",
            "```\n",
            "\n",
            "- **`print(response)`**: This prints the response generated by the language model.\n",
            "\n",
            "### Setting Up the OpenAI API Key\n",
            "\n",
            "Before running the program, you need to set up your OpenAI API key. This is typically done by creating a `.env` file in your project directory with the following content:\n",
            "\n",
            "```\n",
            "OPENAI_API_KEY=your_api_key_here\n",
            "```\n",
            "\n",
            "- Replace `your_api_key_here` with your actual OpenAI API key. The LangChain library will automatically read this file and use the API key for authentication.\n",
            "\n",
            "### Summary\n",
            "\n",
            "1. **Initialization**: The OpenAI language model is initialized with a specified temperature.\n",
            "2. **Prompt Template**: A template for the prompt is created with a placeholder for the message.\n",
            "3. **Chain Creation**: An LLMChain is created to connect the language model with the prompt template.\n",
            "4. **Message Definition**: The message to be sent to the language model is defined.\n",
            "5. **Execution**: The chain is executed with the message, generating a prompt, sending it to the language model, and retrieving the response.\n",
            "6. **Output**: The response from the language model is printed.\n",
            "\n",
            "This program demonstrates a basic workflow for interacting with a language model using LangChain, from initializing the model and creating a prompt template to executing the chain and handling the response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lUjtMuEvNOKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version non standard (ici OpenAI)\n",
        "human_message = HumanMessage(content=[\n",
        "  {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n",
        "  {\"type\": \"image_url\", \"image_url\": {\"url\": \"https: /example.com/image.jpg\"}}\n",
        "])\n",
        "# Version standard avec \"content_blocks\"\n",
        "human_message = HumanMessage(content_blocks=[\n",
        "  {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n",
        "  {\"type\": \"image\", \"url\": \"https: /example.com/image.jpg\"},\n",
        "])\n",
        "# Version non standard (Anthropic)\n",
        "ai_message = AIMessage(\n",
        "content=[\n",
        "  {\"type\": \"thinking\", \"thinking\": \" .\", \"signature\": \"WaUjzkyp .\"},\n",
        "  {\"type\": \"text\", \"text\": \" .\"},\n",
        "  ],\n",
        "  response_metadata={\"model_provider\": \"anthropic\"}\n",
        ")\n",
        "ai_message.content_blocks # LangChain convertit au format standard automatiquement"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN0G1GWrRYrg",
        "outputId": "5823e2d9-ccd8-4af0-a0e7-338274153cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'type': 'reasoning',\n",
              "  'reasoning': ' .',\n",
              "  'extras': {'signature': 'WaUjzkyp .'}},\n",
              " {'type': 'text', 'text': ' .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import XMLOutputParser\n",
        "output_parser = XMLOutputParser(tags=[\"program\"])\n",
        "print(parser.get_format_instructions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPUhHAMiRZOi",
        "outputId": "811ddf31-b93a-41f4-934b-604cc8cf72c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a XML file.\n",
            "1. Output should conform to the tags below.\n",
            "2. If tags are not given, make them on your own.\n",
            "3. Remember to always open and close all the tags.\n",
            "\n",
            "As an example, for the tags [\"foo\", \"bar\", \"baz\"]:\n",
            "1. String \"<foo>\n",
            "   <bar>\n",
            "      <baz></baz>\n",
            "   </bar>\n",
            "</foo>\" is a well-formatted instance of the schema.\n",
            "2. String \"<foo>\n",
            "   <bar>\n",
            "   </foo>\" is a badly-formatted instance.\n",
            "3. String \"<foo>\n",
            "   <tag>\n",
            "   </tag>\n",
            "</foo>\" is a badly-formatted instance.\n",
            "\n",
            "Here are the output tags:\n",
            "```\n",
            "['program']\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffqgkn4dZfyE",
        "outputId": "4feb3e9b-82c8-4a41-daf7-fa95001ead9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.2)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.38)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (8.5.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.11.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt=\"\"\"\n",
        "You are a brilliant Python developer, with a strong knowledge of the most modern syntax of the LangChain framework.\n",
        "You have a good idea of how LLMs work.\n",
        "You are going to be asked to generate programs that communicates with LLMs.\n",
        "Use the LangChain framework. Use the latest syntax with \"Chat Models\".\n",
        "Load API keys using Google Colab secrets using \"userdata\".\n",
        "Include relevant \"%pip install\" commands.\n",
        "Use {llm_provider}.\n",
        "Output the code within XML <program> tags. Output only the code in your response.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate(\n",
        "    [(\"system\", system_prompt), (\"user\", \"{instruction}\")]\n",
        ")\n",
        "\n",
        "# Test what will be the prompt once filled in with variables values\n",
        "prompt_template.invoke({\n",
        "  \"llm_provider\": \"Anthropic\",\n",
        "  \"instruction\": \"Generate a short Python program with LangChain, that sends a message to an LLM.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6x2wvh9XuLM",
        "outputId": "95c1ef3f-c989-4776-a228-dee7fb0dbc4c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='\\nYou are a brilliant Python developer, with a strong knowledge of the most modern syntax of the LangChain framework.\\nYou have a good idea of how LLMs work.\\nYou are going to be asked to generate programs that communicates with LLMs.\\nUse the LangChain framework. Use the latest syntax with \"Chat Models\".\\nLoad API keys using Google Colab secrets using \"userdata\".\\nInclude relevant \"%pip install\" commands.\\nUse Anthropic.\\nOutput the code within XML <program> tags. Output only the code in your response.\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='Generate a short Python program with LangChain, that sends a message to an LLM.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzO2qDwQW-P1"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "# https://mistral.ai/products/la-plateforme#pricing section \"cloud\"\n",
        "model = init_chat_model(\n",
        "    \"codestral-latest\",\n",
        "    model_provider=\"mistralai\", )\n",
        "# LangChain will use the type specific to this provider eg ChatMistralAI\n",
        "print(type(model))\n",
        "chain = prompt_template | model | output_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUQqAVUoXcDC",
        "outputId": "49b66652-21c5-44b8-cc46-88744ce02bd8"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_mistralai.chat_models.ChatMistralAI'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({ \"llm_provider\": \"Anthropic\",\n",
        "  \"instruction\": \"Generate a short Python program with LangChain, that sends a message to an LLM.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxbHEfV9ZYTk",
        "outputId": "fd315b96-8b7a-4c31-8e03-5b6ac7bc1527"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'program': '\\n%pip install langchain anthropic\\n%pip install google-colab\\n\\nimport os\\nfrom langchain.chat_models import ChatAnthropic\\nfrom langchain.schema import HumanMessage\\n\\n# Load API key from Google Colab secrets\\nfrom google.colab import userdata\\nos.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\\'ANTHROPIC_API_KEY\\')\\n\\n# Initialize the ChatAnthropic model\\nchat = ChatAnthropic(model=\"claude-2\")\\n\\n# Create a message\\nmessage = HumanMessage(content=\"Hello, how are you?\")\\n\\n# Send the message to the model\\nresponse = chat([message])\\n\\n# Print the response\\nprint(response.content)\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | model | output_parser | (lambda x: x[\"program\"])"
      ],
      "metadata": {
        "id": "UnssqmCMaTFT"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({ \"llm_provider\": \"Anthropic\",\n",
        "  \"instruction\": \"Generate a short Python program with LangChain, that sends a message to an LLM.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "FHczMZ7Bd-A1",
        "outputId": "1d17ad8f-eb9b-4210-dc36-3fa52aa0537b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n%pip install langchain anthropic\\n\\nfrom langchain.chat_models import ChatAnthropic\\nfrom langchain.schema import HumanMessage\\n\\n# Load API key from Google Colab secrets\\nimport os\\nfrom google.colab import userdata\\nos.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\\'ANTHROPIC_API_KEY\\')\\n\\n# Initialize the ChatAnthropic model\\nchat = ChatAnthropic(model=\"claude-3-opus-20240229\")\\n\\n# Define the message\\nmessage = HumanMessage(\\n    content=\"Translate this sentence from English to French. I love programming.\"\\n)\\n\\n# Send the message to the LLM\\nresponse = chat([message])\\n\\n# Print the response\\nprint(response.content)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "class ProgramSchema(TypedDict):\n",
        "  program: str\n",
        "\n",
        "model_with_output=model.with_structured_output(ProgramSchema)\n",
        "# Plus besoin de \"parser\"\n",
        "program_generator = prompt_template | model_with_output | (lambda x: x[\"program\"])"
      ],
      "metadata": {
        "id": "KRgzizDfeAvW"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "program_generator.invoke({ \"llm_provider\": \"Anthropic\",\n",
        "  \"instruction\": \"Generate a short Python program with LangChain, that sends a message to an LLM.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "reK6AcVHelTH",
        "outputId": "5a79f94e-66f2-4072-dea7-2142b15aba24"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<program>\\n%pip install langchain\\n%pip install anthropic\\n\\nfrom google.colab import userdata\\nfrom langchain_anthropic import ChatAnthropic\\n\\n# Load API key from Google Colab secrets\\napi_key = userdata.get(\\'ANTHROPIC_API_KEY\\')\\n\\n# Initialize the ChatAnthropic model\\nmodel = ChatAnthropic(model=\"claude-3-opus-20240229\", anthropic_api_key=api_key)\\n\\n# Define the message to send to the model\\nmessage = \"Hello, how are you?\"\\n\\n# Send the message to the model and get the response\\nresponse = model.invoke(message)\\n\\n# Print the response\\nprint(response.content)\\n</program>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBspiX76e2hX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}