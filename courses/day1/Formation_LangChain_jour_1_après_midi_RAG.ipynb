{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobain/LBKE_langchain_langgraph/blob/main/day1/notebooks/Formation_LangChain_jour_1_apr%C3%A8s_midi_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swGU8qYw3UZC"
      },
      "source": [
        "# Introduction à LangChain - embeddings et RAG\n",
        "https://www.lbke.fr/formations/developpeur-llm-langgraph-langchain  \n",
        "https://www.lbke.fr/formations/developpeur-llm-langgraph-langchain/cpf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fh8-0cpPdlk"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "iBD3hGOi5ysB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Importing environement variables : will work only this way in colab (harcoded path inside my google drive)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/drive/MyDrive/env_var/.env\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3GTx_4a5KdB",
        "outputId": "50f3553c-a68f-40b5-8f56-c078287dc7db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HjrMwHc186IN"
      },
      "outputs": [],
      "source": [
        "# La syntaxe \"langchain[mistralai]\" installe plusieurs dépendances\n",
        "# Voir la notion d'extras en Python : https://packaging.python.org/en/latest/specifications/dependency-specifiers/#extras\n",
        "%pip install -qU \"langchain[mistralai]\"\n",
        "%pip install -qU langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Hb7pbgx7qXd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SiYF9Abf9Nip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "de099571-7493-49e6-e7e3-1a48b9ff2459"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'formation-intro-langchain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "os.environ[\"LANGSMITH_PROJECT\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHZQlDG-6wZ-",
        "outputId": "181be9fa-8f1f-4a9e-c451-f257301a28dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.38)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.4)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.11.10)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Enabling langsmith\n"
          ]
        }
      ],
      "source": [
        "# We setting LangSmith later in the course but we need this setup to be at top-level\n",
        "# You can disable langsmith if it creates issues or just ignore this cell\n",
        "%pip install langsmith\n",
        "import langsmith as ls\n",
        "print(\"Enabling langsmith\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD5n9rLg9VV0",
        "outputId": "dbfa02ab-36f2-4ea0-ff3b-d4c585ed7aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_mistralai.chat_models.ChatMistralAI'>\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "# https://mistral.ai/products/la-plateforme#pricing section \"cloud\"\n",
        "model = init_chat_model(\"codestral-latest\", model_provider=\"mistralai\", )\n",
        "print(type(model)) # LangChain will use the type specific to this provider eg ChatMistralAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UOL3yqM_9z95"
      },
      "outputs": [],
      "source": [
        "# On vérifie l'accès à Mistral\n",
        "from langchain_core.messages import HumanMessage\n",
        "messages = [\n",
        "    HumanMessage(\"Génère un court programme Python avec LangChain qui envoie un message à un LLM.\"),\n",
        "]\n",
        "res=model.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGrHcbpn-Lk9",
        "outputId": "9c6b4df7-0393-4f79-c0a0-07fd4d94c927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voici un court programme Pytho\n"
          ]
        }
      ],
      "source": [
        "print(res.content[0:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCgcYLnqPe_H"
      },
      "source": [
        "## Prompt engineering\n",
        "(repris du premier notebook d'intro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiblwBw8igDl"
      },
      "source": [
        "### Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22IwC_u5nCq3",
        "outputId": "f0f2d4b6-7459-4a36-d408-f4e905bb7518"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='\\nYou are a brilliant Python developer, with a strong knowledge of the most modern syntax of the LangChain framework.\\nYou have a good idea of how LLMs work.\\nYou are going to be asked to generate programs that communicates with LLMs.\\nUse the LangChain framework. Use the latest syntax with \"Chat Models\".\\nLoad API keys using Google Colab secrets using \"userdata\".\\nInclude relevant \"%pip install\" commands.\\nUse Anthropic.\\nOutput the code within XML <program> tags. Output only the code in your response.\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='Generate a short Python program that sends a message to an LLM.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "system_prompt=\"\"\"\n",
        "You are a brilliant Python developer, with a strong knowledge of the most modern syntax of the LangChain framework.\n",
        "You have a good idea of how LLMs work.\n",
        "You are going to be asked to generate programs that communicates with LLMs.\n",
        "Use the LangChain framework. Use the latest syntax with \"Chat Models\".\n",
        "Load API keys using Google Colab secrets using \"userdata\".\n",
        "Include relevant \"%pip install\" commands.\n",
        "Use {model}.\n",
        "Output the code within XML <program> tags. Output only the code in your response.\n",
        "\"\"\"\n",
        "prompt_template = ChatPromptTemplate(\n",
        "    [(\"system\", system_prompt), (\"user\", \"{instruction}\")],\n",
        "    input_variables=[\"model\", \"instruction\"],\n",
        "    partial_variables={\"model\": \"Mistral AI\"}\n",
        ")\n",
        "prompt_template.invoke({\"model\": \"Anthropic\", \"instruction\": \"Generate a short Python program that sends a message to an LLM.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqiuZGIJud_Y"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YeTZar8uhGV",
        "outputId": "ccf43f4c-eb31-450f-b7e0-adf3458b65c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "# /!\\ Mistral embedding models doesn't support dimensions with \"coarse-to-fine\" approach\n",
        "# as open AI does!\n",
        "embeddings = MistralAIEmbeddings(\n",
        "    model=\"mistral-embed\",\n",
        "    # should match your API limits\n",
        "    max_concurrent_requests=6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3XFpuBHu8Hz",
        "outputId": "27f0b18c-a587-418b-c21a-fc4613c04e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.024749755859375,\n",
              " 0.086181640625,\n",
              " 0.0171966552734375,\n",
              " -0.0212249755859375,\n",
              " 0.055938720703125,\n",
              " 0.0160675048828125,\n",
              " 0.016632080078125,\n",
              " -0.0102081298828125,\n",
              " 0.0184783935546875,\n",
              " -0.01165008544921875]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "embedding=embeddings.embed_query(\"Generate a short Python program that sends a message to an LLM.\")\n",
        "print(len(embedding))\n",
        "embedding[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdgmoZ2yT_4",
        "outputId": "3b32865c-0f07-43cc-a896-c934df6d005c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[np.float64(0.16037343912687763), np.float64(0.27476539864948757)]\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU scipy\n",
        "from scipy.spatial.distance import cosine\n",
        "docs_embeddings=embeddings.embed_documents(\n",
        "    [\"Build a simple LLM application with chat models and prompt templates\",\n",
        "     \"Retrieval Augmented Generation (RAG) Part 1\"])\n",
        "# Distance cosine faible <=> similarité cosine forte\n",
        "# => des embeddings sémantiquement proches\n",
        "print([cosine(embedding,docs_embeddings[0]), cosine(embedding, docs_embeddings[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOd7M7roy2bd",
        "outputId": "ab556393-ce82-4349-e706-b0763339caed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(docs_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py6C_5ra0iNh"
      },
      "source": [
        "## Ingérer des pages web"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_to_parse=\"https://docs.langchain.com/oss/python/langchain/overview\" # new agent API from langchain v1 alpha"
      ],
      "metadata": {
        "id": "tc_m8PZ17Sdh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avec BeautifulSoup"
      ],
      "metadata": {
        "id": "Voox64db6S4T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uTuXyTsg0ltM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28fbf8f-f098-43e2-886a-fe71d4f315c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-docling 1.1.0 requires langchain-core~=0.3.19, but you have langchain-core 1.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-community beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acA01fzo19bs",
        "outputId": "8b8225c1-97e2-456b-ebe0-6cbc4e65d141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# {'source': 'https://docs.langchain.com/oss/python/langchain/overview'}\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4 # beautifulsoup4\n",
        "loader = WebBaseLoader(url_to_parse,\n",
        "                       # pour extraire le tag \"article\" uniquemen\n",
        "                       bs_kwargs={\n",
        "        \"parse_only\": bs4.SoupStrainer(\"article\"),\n",
        "    })\n",
        "docs = loader.load()\n",
        "doc = docs[0]\n",
        "print(f\"# {doc.metadata}\\n\\n\")\n",
        "print(doc.page_content[0:5000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu8boVR_AsOO",
        "outputId": "44bcfe5e-0c89-4bea-b3ae-1caf30c98a7e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview'}, page_content='')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avec docling"
      ],
      "metadata": {
        "id": "v8nWM8106Wcy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SWIkYptR5VQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e619008-3699-46ef-96e2-c57cf2d1db5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-classic 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain 1.0.3 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain-mistralai 1.0.1 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain-community 0.4.1 requires langchain-core<2.0.0,>=1.0.1, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langgraph-prebuilt 1.0.2 requires langchain-core>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain-text-splitters 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-docling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JwGth15i7n37",
        "outputId": "6ad91057-0699-4a04-cffc-7ba87cdcb1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##### LangChain v1.0\n\n- [Release notes](/oss/python/releases/langchain-v1)\n- [Migration guide](/oss/python/migrate/langchain-v1)\n\n##### Get started\n\n- [Install](/oss/python/langchain/install)\n- [Quickstart](/oss/python/langchain/quickstart)\n- [Philosophy](/oss/python/langchain/philosophy)\n\n##### Core components\n\n- [Agents](/oss/python/langchain/agents)\n- [Models](/oss/python/langchain/models)\n- [Messages](/oss/python/langchain/messages)\n- [Tools](/oss/python/langchain/tools)\n- [Short-term memory](/oss/python/langchain/short-term-memory)\n- [Streaming](/oss/python/langchain/streaming)\n- [Middleware](/oss/python/langchain/middleware)\n- [Structured output](/oss/python/langchain/structured-output)\n\n##### Advanced usage\n\n- [Guardrails](/oss/python/langchain/guardrails)\n- [Runtime](/oss/python/langchain/runtime)\n- [Context engineering](/oss/python/langchain/context-engineering)\n- [Model Context Protocol (MCP)](/oss/python/langchain/mcp)\n- [Human-in-the-loop](/oss/python/langchain/human-in-the-loop)\n- [Multi-agent](/oss/python/langchain/multi-agent)\n- [Retrieval](/oss/python/langchain/retrieval)\n- [Long-term memory](/oss/python/langchain/long-term-memory)\n\n##### Use in production\n\n- [Studio](/oss/python/langchain/studio)\n- [Test](/oss/python/langchain/test)\n- [Deploy](/oss/python/langchain/deploy)\n- [Agent Chat UI](/oss/python/langchain/ui)\n- [Observability](/oss/python/langchain/observability)\n\nOn this page\n\n- [Install](#install)\n- [Create an agent](#create-an-agent)\n- [Core benefits](#core-benefits)\n\n# LangChain overview\n\nCopy page\n\nCopy page\n\n**LangChain v1.0 is now available!** For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/python/releases/langchain-v1) and [migration guide](/oss/python/migrate/langchain-v1) . If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=01-langchain.yml) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langchain/tree/v0.3/docs/docs) .\n\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/python/integrations/providers/overview) . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/python/langgraph/overview) , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/python/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## [ Install](#install)\n\npip uv Copy Ask AI\n\n```\npip install -U langchain\n```\n\n## [ Create an agent](#create-an-agent)\n\nCopy Ask AI\n\n```\n# pip install -qU \"langchain[anthropic]\" to call the model\n\nfrom langchain.agents import create_agent\n\ndef get_weather ( city : str ) -> str :\n\"\"\"Get weather for a given city.\"\"\"\nreturn f \"It's always sunny in { city } !\"\n\nagent = create_agent(\nmodel = \"claude-sonnet-4-5-20250929\" ,\ntools = [get_weather],\nsystem_prompt = \"You are a helpful assistant\" ,\n)\n\n# Run the agent\nagent.invoke(\n{ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"what is the weather in sf\" }]}\n)\n```\n\n## [ Core benefits](#core-benefits)\n\n## [Standard model interface](/oss/python/langchain/models)\n\n[Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in. Learn more](/oss/python/langchain/models)\n\n## [Easy to use, highly flexible agent](/oss/python/langchain/agents)\n\n[LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires. Learn more](/oss/python/langchain/agents)\n\n## [Built on top of LangGraph](/oss/python/langgraph/overview)\n\n[LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more. Learn more](/oss/python/langgraph/overview)\n\n## [Debug with LangSmith](/langsmith/home)\n\n[Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics. Learn more](/langsmith/home)\n\n[Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)\n\n[Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.\n\nWas this page helpful?\n\nYes No\n\n[What's new in v1 Next](/oss/python/releases/langchain-v1) ⌘ I"
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Exemple de chargement d'un document\n",
        "from langchain_docling.loader import ExportType, DoclingLoader\n",
        "loader = DoclingLoader(url_to_parse, export_type=ExportType.MARKDOWN)\n",
        "docs = loader.load()\n",
        "doc=docs[0] # If we don't use \"markdown\", we will get multiple document chunks instead\n",
        "print(len(docs))\n",
        "from IPython.display import Markdown\n",
        "Markdown(\"\\n\\n\".join([doc.page_content for doc in docs]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZBrZeNH7ljc"
      },
      "source": [
        "### Découper des documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DSP1jB1U2GSb"
      },
      "outputs": [],
      "source": [
        "# Configuration du splitter\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "# /!\\ MarkdownHeaderTextSplitter ne limite pas la longueur,\n",
        "# TODO: il faudrait le coupler avec un Recursive pour limiter la taille des blocs\n",
        "splitter = MarkdownHeaderTextSplitter([(\"#\", \"title 1\"), (\"##\", \"title 2\"), (\"###\", \"title 3\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qem0FzY3WBl8",
        "outputId": "9c021577-68df-4624-a835-01be3ba858f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 [1529, 1535, 53, 491, 256, 283, 211, 537]\n"
          ]
        }
      ],
      "source": [
        "# Exemple de splitting d'un document\n",
        "chunks=splitter.split_text(doc.page_content)\n",
        "print(len(chunks), [len(chunk.page_content) for chunk in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XkEXOLtnXAcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b77b21-83b2-4d63-bfd7-7d9c930fbcb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  # None\n",
            "  ## None\n",
            "  ### None\n",
            "  ##### LangChain v1.0  \n",
            "- [Release notes](/oss/python/releases/langchain-v1)\n",
            "- [Migration guide](/oss/python/migrate/langchain-v1)  \n",
            "##### Get started  \n",
            "- [Install](/oss/python/langchain/install)\n",
            "- [Quickstart](/oss/python/langchain/quickstart)\n",
            "- [Philosophy](/oss/python/langchain/philosophy)  \n",
            "##### Core components  \n",
            "- [Agents](/oss/python/langchain/agents)\n",
            "- [Models](/oss/python/langchain/models)\n",
            "- [Messages](/oss/python/langchain/messages)\n",
            "- [Tools](/oss/python/langchain/tools)\n",
            "- [Short-term memory](/oss/python/langchain/short-term-memory)\n",
            "- [Streaming](/oss/python/langchain/streaming)\n",
            "- [Middleware](/oss/python/langchain/middleware)\n",
            "- [Structured output](/oss/python/langchain/structured-output)  \n",
            "##### Advanced usage  \n",
            "- [Guardrails](/oss/python/langchain/guardrails)\n",
            "- [Runtime](/oss/python/langchain/runtime)\n",
            "- [Context engineering](/oss/python/langchain/context-engineering)\n",
            "- [Model Context Protocol (MCP)](/oss/python/langchain/mcp)\n",
            "- [Human-in-the-loop](/oss/python/langchain/human-in-the-loop)\n",
            "- [Multi-agent](/oss/python/langchain/multi-agent)\n",
            "- [Retrieval](/oss/python/langchain/retrieval)\n",
            "- [Long-term memory](/oss/python/langchain/long-term-memory)  \n",
            "##### Use in production  \n",
            "- [Studio](/oss/python/langchain/studio)\n",
            "- [Test](/oss/python/langchain/test)\n",
            "- [Deploy](/oss/python/langchain/deploy)\n",
            "- [Agent Chat UI](/oss/python/langchain/ui)\n",
            "- [Observability](/oss/python/langchain/observability)  \n",
            "On this page  \n",
            "- [Install](#install)\n",
            "- [Create an agent](#create-an-agent)\n",
            "- [Core benefits](#core-benefits)\n",
            "  \n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "  # LangChain overview\n",
            "  ## None\n",
            "  ### None\n",
            "  Copy page  \n",
            "Copy page  \n",
            "**LangChain v1.0 is now available!** For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/python/releases/langchain-v1) and [migration guide](/oss/python/migrate/langchain-v1) . If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=01-langchain.yml) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langchain/tree/v0.3/docs/docs) .  \n",
            "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/python/integrations/providers/overview) . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.  \n",
            "We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/python/langgraph/overview) , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.  \n",
            "LangChain [agents](/oss/python/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
            "  \n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "  # LangChain overview\n",
            "  ## [ Install](#install)\n",
            "  ### None\n",
            "  pip uv Copy Ask AI  \n",
            "```\n",
            "pip install -U langchain\n",
            "```\n",
            "  \n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "  # LangChain overview\n",
            "  ## [ Create an agent](#create-an-agent)\n",
            "  ### None\n",
            "  Copy Ask AI  \n",
            "```\n",
            "# pip install -qU \"langchain[anthropic]\" to call the model\n",
            "\n",
            "from langchain.agents import create_agent\n",
            "\n",
            "def get_weather ( city : str ) -> str :\n",
            "\"\"\"Get weather for a given city.\"\"\"\n",
            "return f \"It's always sunny in { city } !\"\n",
            "\n",
            "agent = create_agent(\n",
            "model = \"claude-sonnet-4-5-20250929\" ,\n",
            "tools = [get_weather],\n",
            "system_prompt = \"You are a helpful assistant\" ,\n",
            ")\n",
            "\n",
            "# Run the agent\n",
            "agent.invoke(\n",
            "{ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"what is the weather in sf\" }]}\n",
            ")\n",
            "```\n",
            "  \n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "  # LangChain overview\n",
            "  ## [Standard model interface](/oss/python/langchain/models)\n",
            "  ### None\n",
            "  [Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in. Learn more](/oss/python/langchain/models)\n",
            "  \n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "  # LangChain overview\n",
            "  ## [Easy to use, highly flexible agent](/oss/python/langchain/agents)\n",
            "  ### None\n",
            "  [LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires. Learn more](/oss/python/langchain/agents)\n",
            "  \n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "  # LangChain overview\n",
            "  ## [Built on top of LangGraph](/oss/python/langgraph/overview)\n",
            "  ### None\n",
            "  [LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more. Learn more](/oss/python/langgraph/overview)\n",
            "  \n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "  # LangChain overview\n",
            "  ## [Debug with LangSmith](/langsmith/home)\n",
            "  ### None\n",
            "  [Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics. Learn more](/langsmith/home)  \n",
            "[Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)  \n",
            "[Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.  \n",
            "Was this page helpful?  \n",
            "Yes No  \n",
            "[What's new in v1 Next](/oss/python/releases/langchain-v1) ⌘ I\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Pour le debug\n",
        "from langchain_core.documents import Document\n",
        "def display_chunk(chunk:Document)->str:\n",
        "  return f\"\"\"\n",
        "  # {chunk.metadata.get('title 1')}\n",
        "  ## {chunk.metadata.get('title 2')}\n",
        "  ### {chunk.metadata.get('title 3')}\n",
        "  {chunk.page_content}\n",
        "  \"\"\"\n",
        "print(\"\\n\\n---\\n\\n\".join([display_chunk(chunk) for chunk in chunks]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR6XXpa5Y0Ws"
      },
      "source": [
        "### Fonction d'ingestion des URLs par lots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "o7G-w_mQXFU-"
      },
      "outputs": [],
      "source": [
        "# Code final en combinant toute la logique\n",
        "from langchain_docling.loader import ExportType, DoclingLoader\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# NOTE: we could use a SitemapLoader instead\n",
        "# https://python.langchain.com/docs/integrations/document_loaders/sitemap/\n",
        "def load_urls(urls: List[str])->List[Document]:\n",
        "  chunks=[]\n",
        "  batch_size=4\n",
        "  batches=len(urls)//batch_size\n",
        "  for batch_idx in range(batches+1):\n",
        "    print(f\"Retrieving batch {batch_idx+1} over {batches+1}\")\n",
        "    urls_batch = urls[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
        "    #print(urls_batch)\n",
        "    loader = DoclingLoader(urls_batch, export_type=ExportType.MARKDOWN)\n",
        "    docs = loader.load()\n",
        "    # Les fonctions du splitter peuvent varier selon le type\n",
        "    # Recursive possède une fonction \"split_documents\"\n",
        "    # MarkdownTextHeader n'en possède pas, mais son \"split_text\" renvoie des documents\n",
        "    for doc in docs:\n",
        "      splitted_docs = splitter.split_text(doc.page_content)\n",
        "      chunks = chunks + splitted_docs\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrhDVtRhqV3k",
        "outputId": "5a8214a8-810f-4684-c424-49c8f7fafb2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving batch 1 over 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# Test de la fonction\n",
        "langchain_docs_urls=[\n",
        "    \"https://docs.langchain.com/oss/python/langchain/install\",\n",
        "    \"https://docs.langchain.com/oss/python/langchain/quickstart\",\n",
        "    \"https://docs.langchain.com/oss/python/langchain/agents\",\n",
        "    # \"https://docs.langchain.com/oss/python/langchain/models\"\n",
        "]\n",
        "len(load_urls(langchain_docs_urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "fMoPiG0kDYty"
      },
      "outputs": [],
      "source": [
        "# Bonus: SitemapLoader\n",
        "# https://python.langchain.com/docs/integrations/document_loaders/sitemap/\n",
        "# Attention aux attaques SSRF = urls localhost dans une sitemap\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "# sitemap_url=\"https://python.langchain.com/sitemap.xml\"\n",
        "# from langchain_community.document_loaders import SitemapLoader\n",
        "# sitemap_loader=SitemapLoader(sitemap_url)\n",
        "# sitemap_docs=sitemap_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Rk-v0EdgPMSw"
      },
      "outputs": [],
      "source": [
        "# sitemap_chunks=[]\n",
        "# for doc in sitemap_docs:\n",
        "#   splitted_docs = splitter.split_text(doc.page_content)\n",
        "#   sitemap_chunks = sitemap_chunks + splitted_docs\n",
        "# sitemap_docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iPcYJJ31JPAm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxtSGvzGSedB"
      },
      "source": [
        "## Vector search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "xJ3_rzRT-JCR"
      },
      "outputs": [],
      "source": [
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "# /!\\ Mistral embedding models doesn't support dimensions with \"coarse-to-fine\" approach\n",
        "# as open AI does!\n",
        "embeddings = MistralAIEmbeddings(\n",
        "    model=\"mistral-embed\",\n",
        "    # should match your API limits\n",
        "    max_concurrent_requests=6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "UMH7Syac-QD0"
      },
      "outputs": [],
      "source": [
        "# Open AI alternative\n",
        "# %pip install -qU langchain_openai\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "# embeddings= OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "TXz70fRYVbKf"
      },
      "outputs": [],
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "v-x3clUbVcQw"
      },
      "outputs": [],
      "source": [
        "def ingest_urls(urls:List[str]):\n",
        "  docs = load_urls(urls)\n",
        "  # Mistral has batching out of the box, we just need to properly configure the embedding model\n",
        "  # /!\\ debugging capabilities are limited as we can't observe mistral HTTP requests from LangSmith\n",
        "  # A single failure will fail the whole batch of documents\n",
        "  vector_store.add_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FmEBcRpcgrl",
        "outputId": "bdf89449-1eda-4930-dd51-6ff0ee9a4006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving batch 1 over 1\n"
          ]
        }
      ],
      "source": [
        "# In case of errors, we can debug with logging httpx (used by mistral embedding model) https://www.python-httpx.org/logging/\n",
        "# see https://github.com/langchain-ai/langchain/issues/30524\n",
        "# we can use openai as a fallback\n",
        "ingest_urls(langchain_docs_urls[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "l9nOvzbVI2Xz"
      },
      "outputs": [],
      "source": [
        "# sitemap version\n",
        "# vector_store.add_documents(sitemap_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Dg1DgbJfWLC9"
      },
      "outputs": [],
      "source": [
        "results = vector_store.similarity_search(\"Generate a short Python program that sends a message to an LLM. Use LangChain v1 agent syntax.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkrO2jwIWpBk",
        "outputId": "89857fcf-a836-421a-ebd3-e68f7bb440ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaUdP3NtXB4E",
        "outputId": "6e1f1f8b-0dc4-4b41-bab5-201400ab02a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "{'title 1': 'Agents', 'title 2': '[ Invocation](#invocation)'}\n",
            "{'title 1': 'Quickstart', 'title 2': '[ Build a basic agent](#build-a-basic-agent)'}\n",
            "{'title 1': 'Agents'}\n"
          ]
        }
      ],
      "source": [
        "print('\\n'.join([str(result.metadata) for result in results]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkPGpXlVDpDI"
      },
      "source": [
        "## Un RAG\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "sY7Lv8qXOCVY"
      },
      "outputs": [],
      "source": [
        "# La recherche de document à partir de la requête =>\n",
        "# Première étape de la nouvelle chaîne\n",
        "def augment_query(query):\n",
        "    closest_documents=vector_store.as_retriever(search_kwargs={\"k\": 4}).get_relevant_documents(query)\n",
        "    # Fusion des chaînes de caractère\n",
        "    documents = '\\n---\\n\\n'.join([doc.page_content for doc in closest_documents])\n",
        "    return {\n",
        "    \"instruction\":query,\n",
        "    \"documents\": documents\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(augment_query(\"Generate a short Python program that sends a message to an LLM. Use LangChain v1 agent syntax.\")[\"documents\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BZc3sjXLP8p",
        "outputId": "6d96a020-acde-435f-846d-7cc67388de12"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### LangChain v1.0  \n",
            "- [Release notes](/oss/python/releases/langchain-v1)\n",
            "- [Migration guide](/oss/python/migrate/langchain-v1)  \n",
            "##### Get started  \n",
            "- [Install](/oss/python/langchain/install)\n",
            "- [Quickstart](/oss/python/langchain/quickstart)\n",
            "- [Philosophy](/oss/python/langchain/philosophy)  \n",
            "##### Core components  \n",
            "- [Agents](/oss/python/langchain/agents)\n",
            "- [Models](/oss/python/langchain/models)\n",
            "- [Messages](/oss/python/langchain/messages)\n",
            "- [Tools](/oss/python/langchain/tools)\n",
            "- [Short-term memory](/oss/python/langchain/short-term-memory)\n",
            "- [Streaming](/oss/python/langchain/streaming)\n",
            "- [Middleware](/oss/python/langchain/middleware)\n",
            "- [Structured output](/oss/python/langchain/structured-output)  \n",
            "##### Advanced usage  \n",
            "- [Guardrails](/oss/python/langchain/guardrails)\n",
            "- [Runtime](/oss/python/langchain/runtime)\n",
            "- [Context engineering](/oss/python/langchain/context-engineering)\n",
            "- [Model Context Protocol (MCP)](/oss/python/langchain/mcp)\n",
            "- [Human-in-the-loop](/oss/python/langchain/human-in-the-loop)\n",
            "- [Multi-agent](/oss/python/langchain/multi-agent)\n",
            "- [Retrieval](/oss/python/langchain/retrieval)\n",
            "- [Long-term memory](/oss/python/langchain/long-term-memory)  \n",
            "##### Use in production  \n",
            "- [Studio](/oss/python/langchain/studio)\n",
            "- [Test](/oss/python/langchain/test)\n",
            "- [Deploy](/oss/python/langchain/deploy)\n",
            "- [Agent Chat UI](/oss/python/langchain/ui)\n",
            "- [Observability](/oss/python/langchain/observability)  \n",
            "On this page  \n",
            "- [Build a basic agent](#build-a-basic-agent)\n",
            "- [Build a real-world agent](#build-a-real-world-agent)  \n",
            "[Get started](/oss/python/langchain/install)\n",
            "---\n",
            "\n",
            "You can invoke an agent by passing an update to its [`State`](/oss/python/langgraph/graph-api#state) . All agents include a [sequence of messages](/oss/python/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:  \n",
            "Copy Ask AI  \n",
            "```\n",
            "result = agent.invoke(\n",
            "{ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What's the weather in San Francisco?\" }]}\n",
            ")\n",
            "```  \n",
            "For streaming steps and / or tokens from the agent, refer to the [streaming](/oss/python/langchain/streaming) guide.  \n",
            "Otherwise, the agent follows the LangGraph [Graph API](/oss/python/langgraph/use-graph-api) and supports all associated methods.\n",
            "---\n",
            "\n",
            "Start by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.  \n",
            "For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.  \n",
            "Copy Ask AI  \n",
            "```\n",
            "from langchain.agents import create_agent\n",
            "\n",
            "def get_weather ( city : str ) -> str :\n",
            "\"\"\"Get weather for a given city.\"\"\"\n",
            "return f \"It's always sunny in { city } !\"\n",
            "\n",
            "agent = create_agent(\n",
            "model = \"claude-sonnet-4-5-20250929\" ,\n",
            "tools = [get_weather],\n",
            "system_prompt = \"You are a helpful assistant\" ,\n",
            ")\n",
            "\n",
            "# Run the agent\n",
            "agent.invoke(\n",
            "{ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"what is the weather in sf\" }]}\n",
            ")\n",
            "```  \n",
            "To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain) .\n",
            "---\n",
            "\n",
            "Copy page  \n",
            "Copy page  \n",
            "Agents combine language models with [tools](/oss/python/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.  \n",
            "[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) provides a production-ready agent implementation.  \n",
            "[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/) .  \n",
            "An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.  \n",
            "[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) builds a **graph** -based agent runtime using [LangGraph](/oss/python/langgraph/overview) . A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware. Learn more about the [Graph API](/oss/python/langgraph/graph-api) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "8DV_apbuG12f"
      },
      "outputs": [],
      "source": [
        "# Le template de prompt avec les documents\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "system_prompt=\"\"\"\n",
        "You are a brilliant Python developer, with a strong knowledge of the most modern syntax of the LangChain framework.\n",
        "You have a good idea of how LLMs work.\n",
        "You are going to be asked to generate programs that communicates with LLMs.\n",
        "Use the LangChain framework. Use the latest syntax with \"Chat Models\".\n",
        "Load API keys using Google Colab secrets using \"userdata\".\n",
        "Google Colab secret names are strictly identic to the environment variable name, including casing, for example \"MISTRAL_API_KEY\".\n",
        "Include relevant \"%pip install -qU\" commands.\n",
        "Use {model}.\n",
        "\"\"\"\n",
        "user_prompt = \"\"\"\n",
        "{instruction}\n",
        "\n",
        "Here are documentation elements that can help you:\n",
        "{documents}\n",
        "\"\"\"\n",
        "prompt_template = ChatPromptTemplate(\n",
        "    [(\"system\", system_prompt), (\"user\", user_prompt)],\n",
        "    input_variables=[\"model\", \"instruction\", \"documents\"],\n",
        "    partial_variables={\"model\": \"Mistral AI\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "McSG4cr2Ip5Z"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "class ProgramSchema(TypedDict):\n",
        "    program: str\n",
        "model_with_output=model.with_structured_output(ProgramSchema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ijj4Hr6WFNzD"
      },
      "outputs": [],
      "source": [
        "chain = augment_query | prompt_template | model_with_output | (lambda x: x[\"program\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "8pykBkpoFSmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac3ea13-d297-441c-d3e4-860fe9672250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Install the required packages\n",
            "%pip install -qU langchain langchain-mistralai langgraph\n",
            "\n",
            "# Import necessary modules\n",
            "from langchain_mistralai import ChatMistralAI\n",
            "from langchain.agents import create_agent\n",
            "from langchain_core.messages import HumanMessage\n",
            "\n",
            "# Load the Mistral API key from Google Colab secrets\n",
            "import os\n",
            "from google.colab import userdata\n",
            "\n",
            "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
            "\n",
            "# Initialize the Mistral chat model\n",
            "model = ChatMistralAI(\n",
            "    model=\"mistral-tiny\",  # or another model like \"mistral-small\"\n",
            "    temperature=0.7,\n",
            ")\n",
            "\n",
            "# Define the agent\n",
            "agent = create_agent(\n",
            "    model=model,\n",
            "    # No tools are added for simplicity, but you can add tools here\n",
            ")\n",
            "\n",
            "# Invoke the agent with a message\n",
            "result = agent.invoke({\n",
            "    \"messages\": [\n",
            "        {\n",
            "            \"role\": \"user\",\n",
            "            \"content\": \"Hello! Tell me a fun fact about Python programming.\"\n",
            "        }\n",
            "    ]\n",
            "})\n",
            "\n",
            "# Print the result\n",
            "print(\"Assistant:\", result[\"messages\"][-1].content)\n"
          ]
        }
      ],
      "source": [
        "res = chain.invoke(\"\"\"\"\n",
        "Generate a short Python program that sends a message to an LLM.\n",
        "Use LangChain v1 agent syntax.\n",
        "\"\"\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggiykOn-Otu-",
        "outputId": "e60983e4-8585-4c05-e5cd-88c96405d45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Install the required packages\n",
            "%pip install -qU langchain langchain-mistralai langgraph\n",
            "\n",
            "# Import necessary modules\n",
            "from langchain_mistralai import ChatMistralAI\n",
            "from langchain.agents import create_agent\n",
            "from google.colab import userdata\n",
            "\n",
            "# Load the Mistral API key from Google Colab secrets\n",
            "mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
            "\n",
            "# Define a simple tool for demonstration purposes\n",
            "def greet_user(name: str) -> str:\n",
            "    \"\"\"Greets the user by name.\"\"\"\n",
            "    return f\"Hello, {name}! How can I assist you today?\"\n",
            "\n",
            "# Initialize the Mistral chat model\n",
            "model = ChatMistralAI(\n",
            "    model=\"mistral-large-latest\",\n",
            "    api_key=mistral_api_key\n",
            ")\n",
            "\n",
            "# Create an agent with the Mistral model and the greet_user tool\n",
            "agent = create_agent(\n",
            "    model=model,\n",
            "    tools=[greet_user],\n",
            "    system_prompt=\"You are a helpful assistant. Use the provided tools to assist the user.\"\n",
            ")\n",
            "\n",
            "# Invoke the agent with a user message\n",
            "result = agent.invoke({\n",
            "    \"messages\": [\n",
            "        {\"role\": \"user\", \"content\": \"Greet me. My name is Alice.\"}\n",
            "    ]\n",
            "})\n",
            "\n",
            "# Print the result\n",
            "print(result)\n"
          ]
        }
      ],
      "source": [
        "# Variante bonus avec des runnables\n",
        "# La recherche (version bonus avec un runnable/via le langage LCEL)\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "augment_query_runnable = RunnableParallel({\n",
        "    \"instruction\": RunnablePassthrough(),\n",
        "    \"documents\": vector_store.as_retriever(search_kwargs={\"k\": 4}) | (lambda docs: '\\n\\n'.join([doc.page_content for doc in docs]))\n",
        "    })\n",
        "chain_runnable = augment_query_runnable | prompt_template | model_with_output | (lambda x: x[\"program\"])\n",
        "res_runnable = chain_runnable.invoke(\"Generate a short Python program that sends a message to an LLM. Use LangChain alpha v1 agent syntax\")\n",
        "print(res_runnable)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required packages\n",
        "%pip install -qU langchain langchain-mistralai langgraph\n",
        "\n",
        "# Import necessary modules\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain.agents import create_agent\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the Mistral API key from Google Colab secrets\n",
        "mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "\n",
        "# Define a simple tool for demonstration purposes\n",
        "def greet_user(name: str) -> str:\n",
        "    \"\"\"Greets the user by name.\"\"\"\n",
        "    return f\"Hello, {name}! How can I assist you today?\"\n",
        "\n",
        "# Initialize the Mistral chat model\n",
        "model = ChatMistralAI(\n",
        "    model=\"mistral-large-latest\",\n",
        "    api_key=mistral_api_key\n",
        ")\n",
        "\n",
        "# Create an agent with the Mistral model and the greet_user tool\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[greet_user],\n",
        "    system_prompt=\"You are a helpful assistant. Use the provided tools to assist the user.\"\n",
        ")\n",
        "\n",
        "# Invoke the agent with a user message\n",
        "result = agent.invoke({\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Greet me. My name is Alice.\"}\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Print the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGJCToBtKMOm",
        "outputId": "274d0952-e892-4460-c20a-ebfba16c45d2"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [HumanMessage(content='Greet me. My name is Alice.', additional_kwargs={}, response_metadata={}, id='53a98eef-3da8-43f3-aed8-8edf0ab1c1ef'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'XVRnT9eMN', 'function': {'name': 'greet_user', 'arguments': '{\"name\": \"Alice\"}'}, 'index': 0}]}, response_metadata={'token_usage': {'prompt_tokens': 89, 'total_tokens': 101, 'completion_tokens': 12}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'tool_calls', 'model_provider': 'mistralai'}, id='lc_run--4efe47d3-6a04-435f-a480-cc9c533f821f-0', tool_calls=[{'name': 'greet_user', 'args': {'name': 'Alice'}, 'id': 'XVRnT9eMN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 89, 'output_tokens': 12, 'total_tokens': 101}), ToolMessage(content='Hello, Alice! How can I assist you today?', name='greet_user', id='0c4298c0-dc1a-4e09-81bc-23b60f6da02e', tool_call_id='XVRnT9eMN'), AIMessage(content='Hello, Alice! How can I assist you today? 😊', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 114, 'total_tokens': 129, 'completion_tokens': 15}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop', 'model_provider': 'mistralai'}, id='lc_run--7f3dbad7-8024-48f8-a903-6231cc27e10f-0', usage_metadata={'input_tokens': 114, 'output_tokens': 15, 'total_tokens': 129})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9kEtH5DcKNC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
