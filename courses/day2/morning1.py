# -*- coding: utf-8 -*-
"""Copie de Formation LangGraph - jour 2 agent LangGraph

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aDc0zj9WGYZgTQxqeF-jPDXKhhyVV7yR

# Formation LangGraph - jour 2
https://www.lbke.fr/formations/developpeur-llm-langgraph-langchain
https://www.lbke.fr/formations/developpeur-llm-langgraph-langchain/cpf

## Installation
"""
from dotenv import load_dotenv

load_dotenv()

from langchain.chat_models import init_chat_model

# https://mistral.ai/products/la-plateforme#pricing section "cloud"
model = init_chat_model(
    "codestral-latest",
    model_provider="mistralai",
)
print(type(model))  # LangChain will use the type specific to this provider eg ChatMistralAI

"""## Agents et outils - gestion manuelle avec LangChain

On va découvrir le principe du tool calling
"""

from langchain.tools import tool

import langchain_core  # noqa: F401
import importlib.util


@tool
def langchain_core_exists(value: str) -> bool:
    """
    Check if a namespace or value is exported in the langchain_core module

    Example: langchain_core_exists("ChatOpenAI") # false
    langchain_core_exists(retrievers) # true
    """
    print(f"Check existence of import {value} in langchain_core")
    # https://stackoverflow.com/questions/30483246/how-can-i-check-if-a-module-has-been-imported
    found_spec = importlib.util.find_spec("." + value, package="langchain_core")
    return found_spec is not None


print(langchain_core_exists.invoke("retrievers"))
print(langchain_core_exists.invoke("ChatMistralAI"))

# Le modèle va répondre avec un message de type outil
# Il n'appelle pas encore l'outil !
model_with_tools = model.bind_tools([langchain_core_exists])
ai_message = model_with_tools.invoke("Does 'retrievers' exist in langchain_core module?")
print(ai_message.tool_calls)

# On appelle l'outil pour obtenir le résultat
if not (len(ai_message.tool_calls) and ai_message.tool_calls[0]["name"] == "langchain_core_exists"):
    raise Exception("LLM should have called a langchain_core_exists")
call = ai_message.tool_calls[0]
# Grâce au décorateur "@tool", on peut générer directement un message
tool_message = langchain_core_exists.invoke(call)
import pprint

pprint.pp(tool_message)

# (pprint facilite le copier-coller dans l'interpréteur Python)
import pprint
from langchain_core.messages import HumanMessage

messages = [
    # On pose une question
    HumanMessage("Does 'retrievers' exist in langchain_core module?"),
    # L'IA demande à appeler un outil avant de répondre
    ai_message,
    # On lui donne la réponse de l'outil calculée par nos soins
    tool_message,
]
pprint.pp(messages)
result = model_with_tools.invoke(messages)
# Doit répondre positivement
print(result.content)

# A more difficult prompt
ai_message_mistral = model_with_tools.invoke("Is this code correct : `from langchain_core import ChatMistralAI`?")
# Doit demander un appel d'outil avec ChatMistralAI
pprint.pp(ai_message_mistral)
# Ensuite il faudrait appeler l'outil, donner la réponse de l'outil à l'IA etc.
